{"cells":[{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# !pip install joblib\n\nimport numpy as np\nimport matplotlib.pyplot as plt","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_data(filename):\n    import csv\n    data=[]\n    dataNames=[]\n    with open(filename) as csv_file:\n        csv_reader=csv.reader(csv_file,delimiter=',')\n        load_number=0\n        for row in csv_reader:\n            if load_number==0:\n                dataNames=row\n            else:\n                data.append(row)\n            load_number+=1\n            \n    inputs=[data[i][0] for i in range(len(data))]\n    outputs=[data[i][1] for i in range(len(data))]\n    labelNames=list(set(outputs))\n    \n    return inputs,outputs,labelNames\n\ndef load_dataset(filename):\n    import re\n    data=[]\n    dataNames=[]\n    with open(filename) as file:\n        row=file.readline()\n        while row!='':\n            row=row.split('>')\n            row[1]=str(row[1].split('<')[0])\n            row[0]=row[0].split('<')[1]\n#             print(row)\n            data.append(row)\n            row=file.readline()\n\n    inputs=[data[i][1] for i in range(len(data))]\n    outputs=[data[i][0] for i in range(len(data))]\n    labelNames=list(set(outputs))\n    return inputs,outputs,labelNames","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data\nimport os\n\n# crtDir=os.getcwd()\n# fileName=os.path.join(crtDir,'data','reviews_mixed.csv')\ninputs,outputs,labelNames=load_data('../input/kmeans-diana/data/reviews_mixed.csv')\n\nprint(labelNames)\n","execution_count":21,"outputs":[{"output_type":"stream","text":"['negative', 'positive']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split data\ndef split_data(inputs,outputs):\n    np.random.seed(5)\n    \n    indexes=[i for i in range(len(inputs))]\n    trainSample=np.random.choice(indexes,int(0.8*len(indexes)),replace=False)\n    testSample=[i for i in indexes if i not in trainSample]\n    \n    trainInputs = [inputs[i] for i in trainSample]\n    trainOutputs = [outputs[i] for i in trainSample]\n    testInputs = [inputs[i] for i in testSample]\n    testOutputs = [outputs[i] for i in testSample]\n    \n    return trainInputs,trainOutputs,testInputs,testOutputs\n\ntrainInput,trainOutput,testInput,testOutput=split_data(inputs,outputs)        ","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalise the data using the 3 representations\n# print(trainInput)\n#Bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer_bag=CountVectorizer()\n\ntrainNormalisedInput_bag = vectorizer_bag.fit_transform(trainInput)\ntestNormalisedInput_bag=vectorizer_bag.transform(testInput)\n\n#vocab\n# print('vocab: ', vectorizer_bag.get_feature_names()[:5])\n#features\n# print('features: ' , testNormalisedInput_bag.toarray()[:5])","execution_count":23,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"#Td-idf\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer_tf=TfidfVectorizer(max_features=150)\n\ntrainNormalisedInput_tf = vectorizer_tf.fit_transform(trainInput)\ntestNormalisedInput_tf=vectorizer_tf.transform(testInput)\n\n#vocab=\n# print('vocab: ', vectorizer_tf.get_feature_names()[:5])\n#features\n# print('features: ' , trainNormalisedInput_tf.toarray()[:5])","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install wget\n!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models.word2vec import Word2Vec\nfrom gensim.models import KeyedVectors\n\nword2vecmodel=KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n\n","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def featureComputation(model, data):\n    features = []\n    phrases = [ phrase.split() for phrase in data]\n    for phrase in phrases:\n        # compute the embeddings of all the words from a phrase (words of more than 2 characters) known by the model\n        vectors = [model[word] for word in phrase if (len(word) > 2) and (word in model.vocab.keys())]\n        if len(vectors) == 0:\n            result = [0.0] * model.vector_size\n        else:\n            result = np.sum(vectors, axis=0) / len(vectors)\n        features.append(result)\n    return features\n\ntrainNormalisedInput_word2vec = featureComputation(word2vecmodel, trainInput)\ntestNormalisedInput_word2vec = featureComputation(word2vecmodel, testInput)\n\n# trainNormalisedInput_word2vec=hybrid_normalise(trainInput,trainNormalisedInput_word2vec)\n# testNormalisedInput_word2vec=hybrid_normalise(trainInput,testNormalisedInput_word2vec)\n","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evalMLP(real_labels,computed_labels,label_names):\n    from sklearn.metrics import confusion_matrix\n    \n    cm=confusion_matrix(real_labels,computed_labels)\n    \n    acc=sum([cm[i][i] for i in range(len(label_names))])/len(real_labels)\n    prediction={}\n    recall={}\n    for i in range(len(label_names)):\n        prediction[label_names[i]]=cm[i][i]/sum([cm[j][i] for j in range(len(label_names))])\n        recall[label_names[i]]=cm[i][i]/sum(cm[i][j] for j in range(len(label_names)))\n    \n    return acc,prediction,recall,cm\n\ndef plot_confusion_matrix(cm,class_names,title):\n    import itertools\n    \n    plt.figure(figsize=(12,8))\n    plt.imshow(cm,interpolation='nearest', cmap='Blues')\n    plt.title('The Confusion matrix of '+title)\n    plt.colorbar()\n    \n    tick_marks=np.arange(len(class_names))\n    plt.xticks(tick_marks,class_names,rotation=45)\n    plt.yticks(tick_marks,class_names)\n    \n    #itertools face produse cartezian\n    #daca ar fi itertools.product('ABCD',2)=['AA','AB','AC','AD'..etc]\n    text_format='d'\n    thresh=cm.max()/2\n    for row,column in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n        plt.text(column,row,format(cm[row,column],text_format),horizontalalignment='center', color='white' if cm[row,column]>thresh else 'black')\n        \n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\n    plt.show()","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train and test the data\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#supervised\nfrom sklearn import neural_network\nprint('Supervised:')\n\n#BoW\nprint('BoW')\nsupervised_classifier= neural_network.MLPClassifier(hidden_layer_sizes=(20,2),max_iter=1000,activation='relu',solver='sgd',random_state=1,verbose=0,learning_rate_init=0.1)\n# X_flat=np.array(trainInput)\nsupervised_classifier.fit(trainNormalisedInput_bag,trainOutput)\npredicted_labels=supervised_classifier.predict(testNormalisedInput_bag)\n# print(predicted_labels)\nacc,prediction,recall,cm= evalMLP(np.array(testOutput),predicted_labels,labelNames)\n\n# plot_confusion_matrix(cm,labelNames,'Text classification [negative/positive]')\n\nprint('acc: ',acc)\nprint('prediction: ',prediction)\nprint('recall: ',recall)\n\n# acc:  0.8571428571428571\n# prediction:  {'negative': 0.8518518518518519, 'positive': 0.8666666666666667}\n# recall:  {'negative': 0.92, 'positive': 0.7647058823529411}\n\nprint('Tf-idf')\nsupervised_classifier= neural_network.MLPClassifier(hidden_layer_sizes=(25,2),max_iter=1000,activation='relu',solver='sgd',random_state=1,verbose=0,learning_rate_init=0.1)\n# X_flat=np.array(trainInput)\nsupervised_classifier.fit(trainNormalisedInput_tf,trainOutput)\npredicted_labels=supervised_classifier.predict(testNormalisedInput_tf)\nacc,prediction,recall,cm= evalMLP(np.array(testOutput),predicted_labels,labelNames)\n\n# plot_confusion_matrix(cm,labelNames,'Text classification [negative/positive]')\n\nprint('acc: ',acc)\nprint('prediction: ',prediction)\nprint('recall: ',recall)\n\n# acc:  0.8571428571428571\n# prediction:  {'negative': 0.9130434782608695, 'positive': 0.7894736842105263}\n# recall:  {'negative': 0.84, 'positive': 0.8823529411764706}\n\nprint('Word2vec pre-trained')\nsupervised_classifier= neural_network.MLPClassifier(hidden_layer_sizes=(25,2),max_iter=1000,activation='relu',solver='sgd',random_state=1,verbose=0,learning_rate_init=0.03)\n# supervised_classifier= neural_network.MLPClassifier(hidden_layer_sizes=(25,2),max_iter=1000,activation='relu',solver='sgd',random_state=1,verbose=50,learning_rate_init=0.03)\n# X_flat=np.array(trainInput)\nsupervised_classifier.fit(trainNormalisedInput_word2vec,trainOutput)\npredicted_labels=supervised_classifier.predict(testNormalisedInput_word2vec)\nacc,prediction,recall,cm= evalMLP(np.array(testOutput),predicted_labels,labelNames)\n\n# plot_confusion_matrix(cm,labelNames,'Text classification [negative/positive]')\n\nprint('acc: ',acc)\nprint('prediction: ',prediction)\nprint('recall: ',recall)\n\n# acc:  0.7857142857142857\n# prediction:  {'negative': 0.8636363636363636, 'positive': 0.7}\n# recall:  {'negative': 0.76, 'positive': 0.8235294117647058}","execution_count":41,"outputs":[{"output_type":"stream","text":"Supervised:\nBoW\nacc:  0.8571428571428571\nprediction:  {'negative': 0.8518518518518519, 'positive': 0.8666666666666667}\nrecall:  {'negative': 0.92, 'positive': 0.7647058823529411}\nTf-idf\nacc:  0.8571428571428571\nprediction:  {'negative': 0.9130434782608695, 'positive': 0.7894736842105263}\nrecall:  {'negative': 0.84, 'positive': 0.8823529411764706}\nWord2vec pre-trained\nacc:  0.7857142857142857\nprediction:  {'negative': 0.8636363636363636, 'positive': 0.7}\nrecall:  {'negative': 0.76, 'positive': 0.8235294117647058}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score\n\nclusters=len(labelNames)\nprint('Unsupervised ')\n\nprint('Bag of words')\nunsupervisedClassifier = KMeans(n_clusters=clusters, random_state=0)\nunsupervisedClassifier.fit(trainNormalisedInput_bag)\ncomputedTestIndexes = unsupervisedClassifier.predict(testNormalisedInput_bag)\ncomputedTestOutputs = [labelNames[value] for value in computedTestIndexes]\n\nprint(\"acc: \", accuracy_score(testOutput, computedTestOutputs))\n\nprint('TF-IDF')\nunsupervisedClassifier = KMeans(n_clusters=clusters, random_state=0)\nunsupervisedClassifier.fit(trainNormalisedInput_tf)\ncomputedTestIndexes = unsupervisedClassifier.predict(testNormalisedInput_tf)\ncomputedTestOutputs = [labelNames[value] for value in computedTestIndexes]\n\nprint(\"acc: \", accuracy_score(testOutput, computedTestOutputs))\n\nprint('Word2vec pre-trained')\nunsupervisedClassifier = KMeans(n_clusters=clusters, random_state=0)\nunsupervisedClassifier.fit(trainNormalisedInput_word2vec)\ncomputedTestIndexes = unsupervisedClassifier.predict(testNormalisedInput_word2vec)\ncomputedTestOutputs = [labelNames[value] for value in computedTestIndexes]\n\nprint(\"acc: \", accuracy_score(testOutput, computedTestOutputs))\n\n# Unsupervised \n# Bag of words\n# acc:  0.5714285714285714\n# TF-IDF\n# acc:  0.23809523809523808\n# Word2vec pre-trained\n# acc:  0.40476190476190477\n\n#Hybrid\n# Unsupervised \n# Bag of words\n# acc:  0.42857142857142855\n# TF-IDF\n# acc:  0.7619047619047619\n# Word2vec pre-trained\n# acc:  0.5952380952380952\n    ","execution_count":42,"outputs":[{"output_type":"stream","text":"Unsupervised \nBag of words\nacc:  0.42857142857142855\nTF-IDF\nacc:  0.6904761904761905\nWord2vec pre-trained\nacc:  0.35714285714285715\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}